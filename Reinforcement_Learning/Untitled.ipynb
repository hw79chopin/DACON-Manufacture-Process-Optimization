{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size, input_dims):\n",
    "        self.mem_size = mem_size\n",
    "        self.mem_cntr =0 ## memory counter\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size * input_dims), dtype= np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size * input_dims), dtype = np.float32)\n",
    "        self.action_momery1 = np.zeros(self.mem_size, dtype = np.int32)\n",
    "        self.action_memory2 = np.zeros(self.mem_size, dtype = np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype =np.float32)\n",
    "        self.terminal_memory = np.zeros(sefl.mem_size, dtype =np.in32)\n",
    "        \n",
    "    def store_transition(self, state, action1,action2 reward, state_, done):  #new state\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory1[index] = action1\n",
    "        self.action_memory2[index] = action2\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.terminal_memory[index] = 1 - int(done)\n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace =False)\n",
    "        \n",
    "        states = self.state_memory[batch]\n",
    "        states_ = self.new_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        actions1 = self.action_memory1[batch]\n",
    "        actions2 = self.action_memory2[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "        \n",
    "        return states, actions1, actions2, rewards, states_,terminal\n",
    "    \n",
    "    \n",
    "    def bulid_dqn(lr, n_actions, input_dims, fc1_dims, fc2_dims):\n",
    "        model =keras.Sequential()\n",
    "        model.add(Conv1D(64,8, input_shape = (15,1)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size = 4))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64))\n",
    "        model.add(Dense(n_actions, activation = None))\n",
    "        \n",
    "        model.compile(optimizer = Adam(learning_rate =lr),loss= 'mean_squared_error')\n",
    "        \n",
    "        return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,lr, gamma, n_actions, epilson, batch_size, input_dims, epsilon_dec= 1e-3, epilson_end =0.01,\n",
    "                mem_size = 10000, fname = 'dqn_model.h5'):\n",
    "        self.action = [i for i in range(n_actions)]\n",
    "        self.gamma = gamma\n",
    "        self.epilson = epilson\n",
    "        self.eps_min = epsilon_end\n",
    "        self.eps_dec = epsilon_dec\n",
    "        self.batch_size = batch_size\n",
    "        self.model_file = fname\n",
    "        self.memory = ReplayBuffer(mem_size,input_dims)\n",
    "        self.q_eval = build_dqn(lr, n_actions, input_dims, 256, 256)\n",
    "        \n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else : \n",
    "            state = np.array([observation])\n",
    "            actions = self.q_eval.predict(state)\n",
    "            action = np.argmax(actions)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self) :\n",
    "        if self.memory.mem_cntr < self.batch_size :\n",
    "            return\n",
    "        states, actions, rewards, states_, dones = self.memory.sample_buffer(self.batch_size)\n",
    "        print(states)\n",
    "        \n",
    "        q_eval = self.q_eval.predict(states)\n",
    "        q_next = self.q_eval.predcit(states_)\n",
    "        \n",
    "        q_target = np.copy(q_eval)\n",
    "        batch_index = np.arange(self.batch_size, dtype = np.int32)\n",
    "        \n",
    "        q_target[batch_index, actions] = rewards + self.gamma * np.max(q_next, axis=1)*dones\n",
    "        \n",
    "        self.q_eval_train_on_batch(states, q_target)\n",
    "        self.epsilon =self.epilson - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "        \n",
    "    def save_model(self) :\n",
    "        self.q_eval.save(self.model_file)\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.q_eval = load_model(self.model_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
